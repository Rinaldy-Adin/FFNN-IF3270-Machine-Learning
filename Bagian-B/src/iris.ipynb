{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from activ_func import Activation_Function, reluVect, sigmoidVect, softmax\n",
    "from backprop_func import delta_linear_output, delta_relu_output, delta_sigmoid_output, delta_softmax_output, delta_linear_hidden, delta_relu_hidden, delta_sigmoid_hidden, delta_softmax_hidden\n",
    "\n",
    "ITER_LIMIT = 1000\n",
    "MAX_SSE = 1e-7\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, w: np.ndarray, activ_func: Activation_Function) -> None:\n",
    "        if (w.ndim != 2):\n",
    "            raise RuntimeError(\"Layer initialized with non 2-dimensional array\")\n",
    "\n",
    "        self.w = w\n",
    "        self.n_inputs = w.shape[0]\n",
    "        self.n_neurons = w.shape[1]\n",
    "        self.activ_func = activ_func\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    def __init__(self, n_inputs: int, n_classes: int, learning_rate: float, batch_size: int, max_iter: int, error_threshold: float, stopped_by: str) -> None:\n",
    "        self._n_inputs = n_inputs\n",
    "        self._n_classes = n_classes\n",
    "        self._batch_size = batch_size\n",
    "        self._max_iter = max_iter\n",
    "        self._error_threshold = error_threshold\n",
    "        self._use_max_iter = stopped_by == \"max_iteration\"\n",
    "\n",
    "        self._targets: list[list[float]] = []\n",
    "        self._input: list[list[float]] = []\n",
    "        self._layers: list[Layer] = []\n",
    "        self._batch_grad: list[np.ndarray] = []\n",
    "\n",
    "        self._current_output: np.ndarray = None\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "\n",
    "    def init_batch_grad(self):\n",
    "        self._batch_grad = [np.zeros(shape=(layer.n_inputs, layer.n_neurons)) for layer in self._layers]\n",
    "\n",
    "    def get_output(self):\n",
    "        return np.transpose(self._current_output).tolist()\n",
    "\n",
    "    def addInput(self, newInput: list, target_output: list):\n",
    "        if self._n_inputs != len(newInput):\n",
    "            raise RuntimeError(\"Added input with incorrect number of attributes\")\n",
    "        if self._n_classes != len(target_output):\n",
    "            raise RuntimeError(\"Added target with incorrect number of classes\")\n",
    "\n",
    "        self._input.append(newInput)\n",
    "        self._targets.append(target_output)\n",
    "\n",
    "    def addLayer(self, newLayer: Layer):\n",
    "        if len(self._input) == 0:\n",
    "            raise RuntimeError(\"Input not defined before adding hidden layer\")\n",
    "\n",
    "        if (\n",
    "            len(self._layers) == 0 and (len(self._input[0]) + 1) != newLayer.n_inputs\n",
    "        ) or (len(self._layers) != 0 and (self._layers[-1].n_neurons + 1) != newLayer.n_inputs):\n",
    "            raise RuntimeError(\n",
    "                \"Number of inputs in layer matrix does not match output from previous layer\"\n",
    "            )\n",
    "\n",
    "        self._layers.append(newLayer)\n",
    "\n",
    "    def calc_error(self, output: list, target: list, use_log: bool):\n",
    "        if use_log:\n",
    "            for idx, o_val in enumerate(output):\n",
    "                if target[idx] == 1.0:\n",
    "                    return - np.log(o_val)\n",
    "            return float('inf')\n",
    "        else:\n",
    "            err = 0.0\n",
    "            for idx, o_val in enumerate(output):\n",
    "                err += (target[idx] - o_val) ** 2\n",
    "            err /= 2.0\n",
    "            return err\n",
    "\n",
    "    def feed_forward(self):\n",
    "        for iter in range(ITER_LIMIT):\n",
    "            if iter == self._max_iter:\n",
    "                return\n",
    "\n",
    "            self.init_batch_grad()\n",
    "            iter_error = 0.0\n",
    "\n",
    "            for idx, cur_input in enumerate(self._input):\n",
    "                layer_inputs: list[list[float]] = []\n",
    "                layer_nets: list[list[float]] = []\n",
    "\n",
    "                current = np.transpose(np.array([cur_input]))\n",
    "                bias = np.array([[1.0]])\n",
    "\n",
    "                for _, layer in enumerate(self._layers):\n",
    "                    current = np.concatenate((bias, current), axis=0)\n",
    "                    layer_inputs.append(current.copy().transpose().tolist())\n",
    "\n",
    "                    new_current = np.transpose(layer.w) @ current\n",
    "                    current = new_current\n",
    "                    layer_nets.append(current.copy().transpose().tolist())\n",
    "\n",
    "                    if layer.activ_func == Activation_Function.RELU:\n",
    "                        current = reluVect(current)\n",
    "                    elif layer.activ_func == Activation_Function.SIGMOID:\n",
    "                        current = sigmoidVect(current)\n",
    "                    elif layer.activ_func == Activation_Function.SOFTMAX:\n",
    "                        current = softmax(current)\n",
    "\n",
    "                iter_error += self.calc_error(np.transpose(current).tolist()[0], self._targets[idx], layer.activ_func == Activation_Function.SOFTMAX)\n",
    "                target = self._targets[idx]\n",
    "                self._current_output = current\n",
    "                self.backwards_propagation(layer_inputs, layer_nets, target, iter, idx)\n",
    "\n",
    "                if (idx + 1) % self._batch_size == 0 or idx + 1 == len(self._input):\n",
    "                    self.update_weights()\n",
    "                    self.init_batch_grad()\n",
    "            \n",
    "            if not self._use_max_iter and iter_error <= self._error_threshold:\n",
    "                return\n",
    "\n",
    "        if not self._use_max_iter:\n",
    "            print(\"Using error_threshold to stop but hit ITER_LIMIT to stop program to run indefinitely\")\n",
    "\n",
    "    def update_weights(self):\n",
    "        for idx in range(len(self._layers)):\n",
    "            self._layers[idx].w += self._batch_grad[idx]\n",
    "\n",
    "    def update_batch_grad(self, layer_idx: int, delta: np.ndarray, layer_input: np.ndarray, hidden: bool):\n",
    "        grad = layer_input * delta * self._learning_rate\n",
    "        self._batch_grad[layer_idx] += grad\n",
    "\n",
    "    def backwards_propagation(self, layer_inputs: list[list[float]], layer_nets: list[list[float]], target: list[float], iter: int, input_idx: int):\n",
    "        ds_delta: np.ndarray = None\n",
    "        for idx, layer in enumerate(reversed(self._layers)):\n",
    "            layer_idx = (-1-idx) % len(self._layers)\n",
    "            nets = np.array(layer_nets[layer_idx]).transpose()\n",
    "\n",
    "            if idx == 0:\n",
    "                target_mat = np.array(target).transpose()\n",
    "\n",
    "                if layer.activ_func == Activation_Function.SOFTMAX:\n",
    "                    ds_delta = delta_softmax_output(self._current_output, target_mat)\n",
    "                elif layer.activ_func == Activation_Function.RELU:\n",
    "                    ds_delta = delta_relu_output(self._current_output, target_mat, nets)\n",
    "                elif layer.activ_func == Activation_Function.SIGMOID:\n",
    "                    ds_delta = delta_sigmoid_output(self._current_output, target_mat)\n",
    "                else:\n",
    "                    ds_delta = delta_linear_output(self._current_output, target_mat)\n",
    "            else:\n",
    "                cur_delta = None\n",
    "                layer_outputs = np.array([layer_inputs[layer_idx + 1][0][1:]])\n",
    "\n",
    "                if layer.activ_func == Activation_Function.SOFTMAX:\n",
    "                    cur_delta = delta_softmax_hidden(layer_outputs, ds_delta, self._layers[layer_idx + 1].w)\n",
    "                elif layer.activ_func == Activation_Function.RELU:\n",
    "                    cur_delta = delta_relu_hidden(nets, ds_delta, self._layers[layer_idx + 1].w)\n",
    "                elif layer.activ_func == Activation_Function.SIGMOID:\n",
    "                    cur_delta = delta_sigmoid_hidden(layer_outputs, ds_delta, self._layers[layer_idx + 1].w)\n",
    "                else:\n",
    "                    cur_delta = delta_linear_hidden(ds_delta, self._layers[layer_idx + 1].w)\n",
    "\n",
    "                ds_layer_input = np.array(layer_inputs[layer_idx + 1]).transpose()\n",
    "                self.update_batch_grad(layer_idx + 1, ds_delta,ds_layer_input, False)\n",
    "                ds_delta = cur_delta\n",
    "\n",
    "        ds_layer_input = np.array(layer_inputs[0]).transpose()\n",
    "        self.update_batch_grad(0, ds_delta, ds_layer_input, True)\n",
    "\n",
    "    def predict(self, input_data: list[float]):\n",
    "        if len(input_data) != self._n_inputs:\n",
    "            raise RuntimeError(\"Input to predict has incorrect number of attributes\")\n",
    "\n",
    "        current = np.array([input_data]).transpose()\n",
    "        bias = np.array([[1.0]])\n",
    "\n",
    "        for layer in self._layers:\n",
    "            current = np.concatenate((bias, current), axis=0)\n",
    "\n",
    "            new_current = np.transpose(layer.w) @ current\n",
    "            current = new_current\n",
    "\n",
    "            if layer.activ_func == Activation_Function.RELU:\n",
    "                current = reluVect(current)\n",
    "            elif layer.activ_func == Activation_Function.SIGMOID:\n",
    "                current = sigmoidVect(current)\n",
    "            elif layer.activ_func == Activation_Function.SOFTMAX:\n",
    "                current = softmax(current)\n",
    "\n",
    "        output = current.transpose().tolist()[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.1, 3.5, 1.4, 0.2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\TB_ML_A\\FFNN-IF3270-Machine-Learning\\Bagian-B\\src\\activ_func.py:41: RuntimeWarning: overflow encountered in exp\n",
      "  row_exp_sum = np.sum(np.exp(row))\n",
      "c:\\Users\\ASUS\\TB_ML_A\\FFNN-IF3270-Machine-Learning\\Bagian-B\\src\\activ_func.py:24: RuntimeWarning: overflow encountered in exp\n",
      "  e_v = np.exp(v)\n",
      "c:\\Users\\ASUS\\TB_ML_A\\FFNN-IF3270-Machine-Learning\\Bagian-B\\src\\activ_func.py:25: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return e_v / expSum\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\lib\\function_base.py:2411: RuntimeWarning: invalid value encountered in softmaxElmt (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan, nan, nan]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('../data/iris.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    iris_data = list(reader)[1:]\n",
    "\n",
    "w_1 = np.random.seed(0)\n",
    "\n",
    "w_1 = np.random.uniform(low=-0.5, high=0.5, size=(5, 4))\n",
    "w_2 = np.random.uniform(low=-0.5, high=0.5, size=(5, 4))\n",
    "w_3 = np.random.uniform(low=-0.5, high=0.5, size=(5, 4))\n",
    "w_final = np.random.uniform(low=-0.5, high=0.5, size=(5, 3))\n",
    "\n",
    "input_size = 4\n",
    "n_classes = 3\n",
    "learning_rate = 0.2\n",
    "batch_size = 1\n",
    "max_iter = 2\n",
    "error_threshold = 0.0\n",
    "stopped_by = \"max_iteration\"\n",
    "\n",
    "fnaf = FFNN(input_size, n_classes, learning_rate, batch_size, max_iter, error_threshold, stopped_by)\n",
    "\n",
    "for idx, row in enumerate(iris_data):\n",
    "    input_data = row[1:-1]\n",
    "    input_data = [float(x) for x in input_data]\n",
    "    target_data = [0.0, 0.0, 0.0]\n",
    "\n",
    "    if row[-1] == \"Iris-setosa\":\n",
    "        target_data[0] = 1.0\n",
    "    elif row[-1] == \"Iris-versicolor\":\n",
    "        target_data[1] = 1.0\n",
    "    else:\n",
    "        target_data[2] = 1.0\n",
    "\n",
    "    fnaf.addInput(input_data, target_data)\n",
    "\n",
    "fnaf.addLayer(Layer(w_1, Activation_Function.LINEAR))\n",
    "fnaf.addLayer(Layer(w_final, Activation_Function.SOFTMAX))\n",
    "\n",
    "fnaf.feed_forward()\n",
    "\n",
    "test_in = iris_data[0][1:-1]\n",
    "test_in = [float(x) for x in test_in]\n",
    "print(test_in)\n",
    "fnaf.predict(test_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.12135773\n",
      "Iteration 2, loss = 2.09521495\n",
      "Iteration 3, loss = 2.06943994\n",
      "Iteration 4, loss = 2.04403624\n",
      "Iteration 5, loss = 2.01906487\n",
      "Iteration 6, loss = 1.99445333\n",
      "Iteration 7, loss = 1.97021547\n",
      "Iteration 8, loss = 1.94635058\n",
      "Iteration 9, loss = 1.92284543\n",
      "Iteration 10, loss = 1.89971574\n",
      "Iteration 11, loss = 1.87699711\n",
      "Iteration 12, loss = 1.85466862\n",
      "Iteration 13, loss = 1.83269246\n",
      "Iteration 14, loss = 1.81106442\n",
      "Iteration 15, loss = 1.78978560\n",
      "Iteration 16, loss = 1.76886166\n",
      "Iteration 17, loss = 1.74829291\n",
      "Iteration 18, loss = 1.72808279\n",
      "Iteration 19, loss = 1.70820938\n",
      "Iteration 20, loss = 1.68867652\n",
      "Iteration 21, loss = 1.66948563\n",
      "Iteration 22, loss = 1.65064000\n",
      "Iteration 23, loss = 1.63211006\n",
      "Iteration 24, loss = 1.61390315\n",
      "Iteration 25, loss = 1.59600765\n",
      "Iteration 26, loss = 1.57842538\n",
      "Iteration 27, loss = 1.56114493\n",
      "Iteration 28, loss = 1.54417867\n",
      "Iteration 29, loss = 1.52752306\n",
      "Iteration 30, loss = 1.51115987\n",
      "Iteration 31, loss = 1.49509509\n",
      "Iteration 32, loss = 1.47930647\n",
      "Iteration 33, loss = 1.46378532\n",
      "Iteration 34, loss = 1.44853786\n",
      "Iteration 35, loss = 1.43357303\n",
      "Iteration 36, loss = 1.41887256\n",
      "Iteration 37, loss = 1.40443920\n",
      "Iteration 38, loss = 1.39026371\n",
      "Iteration 39, loss = 1.37634323\n",
      "Iteration 40, loss = 1.36267933\n",
      "Iteration 41, loss = 1.34928150\n",
      "Iteration 42, loss = 1.33614368\n",
      "Iteration 43, loss = 1.32324191\n",
      "Iteration 44, loss = 1.31058715\n",
      "Iteration 45, loss = 1.29815974\n",
      "Iteration 46, loss = 1.28594740\n",
      "Iteration 47, loss = 1.27394741\n",
      "Iteration 48, loss = 1.26215699\n",
      "Iteration 49, loss = 1.25057744\n",
      "Iteration 50, loss = 1.23921555\n",
      "Iteration 51, loss = 1.22805586\n",
      "Iteration 52, loss = 1.21709791\n",
      "Iteration 53, loss = 1.20633167\n",
      "Iteration 54, loss = 1.19575805\n",
      "Iteration 55, loss = 1.18536825\n",
      "Iteration 56, loss = 1.17516569\n",
      "Iteration 57, loss = 1.16515291\n",
      "Iteration 58, loss = 1.15531643\n",
      "Iteration 59, loss = 1.14564970\n",
      "Iteration 60, loss = 1.13615419\n",
      "Iteration 61, loss = 1.12682844\n",
      "Iteration 62, loss = 1.11767392\n",
      "Iteration 63, loss = 1.10869134\n",
      "Iteration 64, loss = 1.09987158\n",
      "Iteration 65, loss = 1.09121913\n",
      "Iteration 66, loss = 1.08272913\n",
      "Iteration 67, loss = 1.07439468\n",
      "Iteration 68, loss = 1.06621757\n",
      "Iteration 69, loss = 1.05818792\n",
      "Iteration 70, loss = 1.05030437\n",
      "Iteration 71, loss = 1.04256349\n",
      "Iteration 72, loss = 1.03496094\n",
      "Iteration 73, loss = 1.02748596\n",
      "Iteration 74, loss = 1.02014716\n",
      "Iteration 75, loss = 1.01294297\n",
      "Iteration 76, loss = 1.00586137\n",
      "Iteration 77, loss = 0.99890581\n",
      "Iteration 78, loss = 0.99207146\n",
      "Iteration 79, loss = 0.98535968\n",
      "Iteration 80, loss = 0.97875297\n",
      "Iteration 81, loss = 0.97225897\n",
      "Iteration 82, loss = 0.96587144\n",
      "Iteration 83, loss = 0.95958763\n",
      "Iteration 84, loss = 0.95340875\n",
      "Iteration 85, loss = 0.94732484\n",
      "Iteration 86, loss = 0.94134062\n",
      "Iteration 87, loss = 0.93545471\n",
      "Iteration 88, loss = 0.92967004\n",
      "Iteration 89, loss = 0.92398861\n",
      "Iteration 90, loss = 0.91839495\n",
      "Iteration 91, loss = 0.91288481\n",
      "Iteration 92, loss = 0.90746102\n",
      "Iteration 93, loss = 0.90211502\n",
      "Iteration 94, loss = 0.89684839\n",
      "Iteration 95, loss = 0.89166056\n",
      "Iteration 96, loss = 0.88655610\n",
      "Iteration 97, loss = 0.88152482\n",
      "Iteration 98, loss = 0.87655895\n",
      "Iteration 99, loss = 0.87165995\n",
      "Iteration 100, loss = 0.86683489\n",
      "Iteration 101, loss = 0.86208430\n",
      "Iteration 102, loss = 0.85739418\n",
      "Iteration 103, loss = 0.85276375\n",
      "Iteration 104, loss = 0.84819853\n",
      "Iteration 105, loss = 0.84369529\n",
      "Iteration 106, loss = 0.83923971\n",
      "Iteration 107, loss = 0.83482190\n",
      "Iteration 108, loss = 0.83045375\n",
      "Iteration 109, loss = 0.82614089\n",
      "Iteration 110, loss = 0.82188229\n",
      "Iteration 111, loss = 0.81767703\n",
      "Iteration 112, loss = 0.81351732\n",
      "Iteration 113, loss = 0.80940748\n",
      "Iteration 114, loss = 0.80535323\n",
      "Iteration 115, loss = 0.80134759\n",
      "Iteration 116, loss = 0.79738955\n",
      "Iteration 117, loss = 0.79348933\n",
      "Iteration 118, loss = 0.78963910\n",
      "Iteration 119, loss = 0.78583336\n",
      "Iteration 120, loss = 0.78206772\n",
      "Iteration 121, loss = 0.77834310\n",
      "Iteration 122, loss = 0.77465932\n",
      "Iteration 123, loss = 0.77101603\n",
      "Iteration 124, loss = 0.76740888\n",
      "Iteration 125, loss = 0.76384023\n",
      "Iteration 126, loss = 0.76030829\n",
      "Iteration 127, loss = 0.75681184\n",
      "Iteration 128, loss = 0.75334832\n",
      "Iteration 129, loss = 0.74991505\n",
      "Iteration 130, loss = 0.74650829\n",
      "Iteration 131, loss = 0.74313283\n",
      "Iteration 132, loss = 0.73979058\n",
      "Iteration 133, loss = 0.73648239\n",
      "Iteration 134, loss = 0.73320309\n",
      "Iteration 135, loss = 0.72995432\n",
      "Iteration 136, loss = 0.72673610\n",
      "Iteration 137, loss = 0.72354272\n",
      "Iteration 138, loss = 0.72037313\n",
      "Iteration 139, loss = 0.71722494\n",
      "Iteration 140, loss = 0.71409419\n",
      "Iteration 141, loss = 0.71098752\n",
      "Iteration 142, loss = 0.70790128\n",
      "Iteration 143, loss = 0.70483172\n",
      "Iteration 144, loss = 0.70177585\n",
      "Iteration 145, loss = 0.69874334\n",
      "Iteration 146, loss = 0.69572767\n",
      "Iteration 147, loss = 0.69272443\n",
      "Iteration 148, loss = 0.68973867\n",
      "Iteration 149, loss = 0.68676434\n",
      "Iteration 150, loss = 0.68380682\n",
      "Iteration 151, loss = 0.68086425\n",
      "Iteration 152, loss = 0.67793588\n",
      "Iteration 153, loss = 0.67502202\n",
      "Iteration 154, loss = 0.67212849\n",
      "Iteration 155, loss = 0.66924534\n",
      "Iteration 156, loss = 0.66637727\n",
      "Iteration 157, loss = 0.66352433\n",
      "Iteration 158, loss = 0.66068735\n",
      "Iteration 159, loss = 0.65786276\n",
      "Iteration 160, loss = 0.65505720\n",
      "Iteration 161, loss = 0.65226036\n",
      "Iteration 162, loss = 0.64947660\n",
      "Iteration 163, loss = 0.64670711\n",
      "Iteration 164, loss = 0.64395051\n",
      "Iteration 165, loss = 0.64120424\n",
      "Iteration 166, loss = 0.63846817\n",
      "Iteration 167, loss = 0.63574040\n",
      "Iteration 168, loss = 0.63302192\n",
      "Iteration 169, loss = 0.63031225\n",
      "Iteration 170, loss = 0.62760837\n",
      "Iteration 171, loss = 0.62490460\n",
      "Iteration 172, loss = 0.62220543\n",
      "Iteration 173, loss = 0.61951231\n",
      "Iteration 174, loss = 0.61682813\n",
      "Iteration 175, loss = 0.61415957\n",
      "Iteration 176, loss = 0.61150334\n",
      "Iteration 177, loss = 0.60885590\n",
      "Iteration 178, loss = 0.60621391\n",
      "Iteration 179, loss = 0.60357880\n",
      "Iteration 180, loss = 0.60095215\n",
      "Iteration 181, loss = 0.59833056\n",
      "Iteration 182, loss = 0.59571583\n",
      "Iteration 183, loss = 0.59310498\n",
      "Iteration 184, loss = 0.59049986\n",
      "Iteration 185, loss = 0.58790059\n",
      "Iteration 186, loss = 0.58530869\n",
      "Iteration 187, loss = 0.58272842\n",
      "Iteration 188, loss = 0.58015315\n",
      "Iteration 189, loss = 0.57758515\n",
      "Iteration 190, loss = 0.57502577\n",
      "Iteration 191, loss = 0.57247318\n",
      "Iteration 192, loss = 0.56992815\n",
      "Iteration 193, loss = 0.56738943\n",
      "Iteration 194, loss = 0.56485436\n",
      "Iteration 195, loss = 0.56231992\n",
      "Iteration 196, loss = 0.55979114\n",
      "Iteration 197, loss = 0.55726922\n",
      "Iteration 198, loss = 0.55475412\n",
      "Iteration 199, loss = 0.55224121\n",
      "Iteration 200, loss = 0.54973252\n",
      "Accuracy: 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"../data/iris.csv\")\n",
    "df_x = data.iloc[:, 1:5]\n",
    "df_y = data.iloc[:, 5]\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df_y)\n",
    "\n",
    "# Convert integers to one-hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, onehot_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(x_train)\n",
    "X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Train MLPClassifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=42, verbose=1)\n",
    "mlp_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = mlp_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
