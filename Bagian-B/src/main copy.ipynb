{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with TensorFlow Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "from activ_func import Activation_Function, reluVect, sigmoidVect, softmax\n",
    "from backprop_func import delta_linear_output, delta_relu_output, delta_sigmoid_output, delta_softmax_output, delta_linear_hidden, delta_relu_hidden, delta_sigmoid_hidden, delta_softmax_hidden\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, w: np.ndarray, activ_func: Activation_Function) -> None:\n",
    "        if (w.ndim != 2):\n",
    "            raise RuntimeError(\"Layer initialized with non 2-dimensional array\")\n",
    "\n",
    "        self.w = w\n",
    "        self.n_inputs = w.shape[0]\n",
    "        self.n_neurons = w.shape[1]\n",
    "        self.activ_func = activ_func\n",
    "        \n",
    "    def getWeight(self):\n",
    "        return self.w\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    def __init__(self, n_inputs: int, n_classes: int, learning_rate: float) -> None:\n",
    "        self._n_inputs = n_inputs\n",
    "        self._n_classes = n_classes\n",
    "\n",
    "        self._targets: list[list[float]] = []\n",
    "        self._input: list[list[float]] = []\n",
    "        self._layers: list[Layer] = []\n",
    "\n",
    "        self._current_output: np.ndarray = None\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "\n",
    "    def get_output(self):\n",
    "        return np.transpose(self._current_output).tolist()\n",
    "\n",
    "    def addInput(self, newInput: list[float], target_output: list[float]):\n",
    "        if self._n_inputs != len(newInput):\n",
    "            raise RuntimeError(\"Added input with incorrect number of attributes\")\n",
    "        if self._n_classes != len(target_output):\n",
    "            raise RuntimeError(\"Added target with incorrect number of classes\")\n",
    "\n",
    "        self._input.append(newInput)\n",
    "        self._targets.append(target_output)\n",
    "\n",
    "    def addLayer(self, newLayer: Layer):\n",
    "        if len(self._input) == 0:\n",
    "            raise RuntimeError(\"Input not defined before adding hidden layer\")\n",
    "\n",
    "        if (\n",
    "            len(self._layers) == 0 and (len(self._input[0]) + 1) != newLayer.n_inputs\n",
    "        ) or (len(self._layers) != 0 and (self._layers[-1].n_neurons + 1) != newLayer.n_inputs):\n",
    "            raise RuntimeError(\n",
    "                \"Number of inputs in layer matrix does not match output from previous layer\"\n",
    "            )\n",
    "\n",
    "        self._layers.append(newLayer)\n",
    "\n",
    "    def feed_forward(self):\n",
    "        for cur_input in self._input:\n",
    "            layer_inputs: list[list[float]] = []\n",
    "            layer_nets: list[list[float]] = []\n",
    "\n",
    "            current = np.transpose(np.array([cur_input]))\n",
    "            bias = np.array([[1.0]])\n",
    "\n",
    "            for _, layer in enumerate(self._layers):\n",
    "                current = np.concatenate((bias, current), axis=0)\n",
    "                layer_inputs.append(current.copy().transpose().tolist())\n",
    "\n",
    "                new_current = np.transpose(layer.w) @ current\n",
    "                current = new_current\n",
    "                layer_nets.append(current.copy().transpose().tolist())\n",
    "\n",
    "                if layer.activ_func == Activation_Function.RELU:\n",
    "                    current = reluVect(current)\n",
    "                elif layer.activ_func == Activation_Function.SIGMOID:\n",
    "                    current = sigmoidVect(current)\n",
    "                elif layer.activ_func == Activation_Function.SOFTMAX:\n",
    "                    current = softmax(current)\n",
    "\n",
    "            self._current_output = current\n",
    "            self.backwards_propagation(layer_inputs, layer_nets)\n",
    "\n",
    "    def update_w(self, layer_idx: int, delta: np.ndarray, inputs: list[float]):\n",
    "        input_mat = np.transpose(np.array(inputs))\n",
    "        input_mat = np.tile(input_mat, (1, self._layers[layer_idx].n_neurons))\n",
    "        self._layers[layer_idx].w += self._learning_rate * delta * input_mat\n",
    "\n",
    "    def backwards_propagation(self, layer_inputs: list[list[float]], layer_nets: list[list[float]]):\n",
    "        ds_delta: np.ndarray = None\n",
    "        for idx, layer in enumerate(reversed(self._layers)):\n",
    "            layer_idx = (-1-idx) % len(self._layers)\n",
    "            nets = np.array(layer_nets[layer_idx]).transpose()\n",
    "\n",
    "            if idx == 0:\n",
    "                target = np.array(self._targets[layer_idx]).transpose()\n",
    "\n",
    "                if layer.activ_func == Activation_Function.SOFTMAX:\n",
    "                    ds_delta = delta_softmax_output(self._current_output, target, layer.n_inputs)\n",
    "                elif layer.activ_func == Activation_Function.RELU:\n",
    "                    ds_delta = delta_relu_output(self._current_output, target, nets, layer.n_inputs)\n",
    "                elif layer.activ_func == Activation_Function.SIGMOID:\n",
    "                    ds_delta = delta_sigmoid_output(self._current_output, target, layer.n_inputs)\n",
    "                else:\n",
    "                    ds_delta = delta_linear_output(self._current_output, target, layer.n_inputs)\n",
    "            else:\n",
    "                cur_delta = None\n",
    "                layer_outputs = np.array(layer_inputs[layer_idx + 1][1:])\n",
    "\n",
    "                if layer.activ_func == Activation_Function.SOFTMAX:\n",
    "                    cur_delta = delta_softmax_hidden(layer_outputs, ds_delta, self._layers[layer_idx + 1].w, layer.n_inputs)\n",
    "                elif layer.activ_func == Activation_Function.RELU:\n",
    "                    cur_delta = delta_relu_hidden(nets, ds_delta, self._layers[layer_idx + 1].w, layer.n_inputs)\n",
    "                elif layer.activ_func == Activation_Function.SIGMOID:\n",
    "                    cur_delta = delta_sigmoid_hidden(layer_outputs, ds_delta, self._layers[layer_idx + 1].w, layer.n_inputs)\n",
    "                else:\n",
    "                    cur_delta = delta_linear_hidden(ds_delta, self._layers[layer_idx + 1].w, layer.n_inputs)\n",
    "\n",
    "                self.update_w(layer_idx + 1, ds_delta, layer_inputs[layer_idx + 1])\n",
    "                ds_delta = cur_delta\n",
    "    def getLayers(self):\n",
    "        return self._layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def d_relu(v: float, net: float):\n",
    "    return 0 if net < 0 else v\n",
    "\n",
    "d_relu_vect = np.vectorize(d_relu)\n",
    "\n",
    "def d_softmax(o: float, net: float):\n",
    "    return -o if net == 1.0 else 1-0\n",
    "\n",
    "d_softmax_vect = np.vectorize(d_softmax)\n",
    "\n",
    "\"\"\"\n",
    "All variables output, target, nets are in the form of\n",
    "single column matrix\n",
    "\"\"\"\n",
    "\n",
    "def delta_linear_output(output: np.ndarray, target: np.ndarray, n_inputs: int):\n",
    "    output_mat = np.tile(np.transpose(output), reps=(n_inputs, 1))\n",
    "    target_mat = np.tile(np.transpose(target), reps=(n_inputs, 1))\n",
    "    return target_mat - output_mat\n",
    "\n",
    "def delta_relu_output(output: np.ndarray, target: np.ndarray, nets: np.ndarray, n_inputs: int):\n",
    "    output_mat = np.tile(np.transpose(output), reps=(n_inputs, 1))\n",
    "    target_mat = np.tile(np.transpose(target), reps=(n_inputs, 1))\n",
    "    nets_mat = np.tile(np.transpose(nets), reps=(n_inputs, 1))\n",
    "\n",
    "    return d_relu_vect(target_mat - output_mat, nets_mat)\n",
    "\n",
    "def delta_sigmoid_output(output: np.ndarray, target: np.ndarray, n_inputs: int):\n",
    "    output_mat = np.tile(np.transpose(output), reps=(n_inputs, 1))\n",
    "    target_mat = np.tile(np.transpose(target), reps=(n_inputs, 1))\n",
    "    \n",
    "    return (target_mat - output_mat) * output_mat * (1 - output_mat)\n",
    "\n",
    "def delta_softmax_output(output: np.ndarray, target: np.ndarray, n_inputs: int):\n",
    "    output_mat = np.tile(np.transpose(output), reps=(n_inputs, 1))\n",
    "    target_mat = np.tile(np.transpose(target), reps=(n_inputs, 1))\n",
    "\n",
    "    return d_softmax_vect(output_mat, target_mat)\n",
    "\n",
    "def delta_linear_hidden(\n",
    "    ds_delta: np.ndarray, ds_w: np.ndarray, n_inputs: int\n",
    "):\n",
    "    sigma_vect = np.array([[np.sum(ds_delta[row_idx] * ds_w[row_idx]) for row_idx in range(ds_w.shape[0])]])\n",
    "\n",
    "    return np.tile(sigma_vect, reps=(n_inputs, 1))\n",
    "\n",
    "def delta_relu_hidden(\n",
    "    nets: np.ndarray, ds_delta: np.ndarray, ds_w: np.ndarray, n_inputs: int\n",
    "):\n",
    "    nets_vect = np.transpose(nets)\n",
    "    # TODO check if this handles bias correctly\n",
    "    sigma_vect = np.array([[np.sum(ds_delta[row_idx] * ds_w[row_idx]) for row_idx in range(ds_w.shape[0])]])\n",
    "\n",
    "    return np.tile(d_relu_vect(sigma_vect, nets_vect), reps=(n_inputs, 1))\n",
    "\n",
    "\n",
    "def delta_sigmoid_hidden(\n",
    "    output: np.ndarray, ds_delta: np.ndarray, ds_w: np.ndarray, n_inputs: int\n",
    "):\n",
    "    output_vect = np.transpose(output)\n",
    "    output_vect = output_vect * (1 - output_vect)\n",
    "\n",
    "    sigma_vect = np.array([[np.sum(ds_delta[row_idx] * ds_w[row_idx]) for row_idx in range(ds_w.shape[0])]])\n",
    "\n",
    "    return np.tile(output_vect * sigma_vect, reps=(n_inputs, 1))\n",
    "\n",
    "def delta_softmax_hidden(\n",
    "    output: np.ndarray, ds_delta: np.ndarray, ds_w: np.ndarray, n_inputs: int\n",
    "):\n",
    "    output_deltas = []\n",
    "    o_list: list[float] = np.transpose(output).tolist()[0]\n",
    "\n",
    "    ds_sums: list[float] = [np.sum(ds_delta[j] * ds_w[j]) for j in range(len(ds_delta))]\n",
    "\n",
    "    for i, oi in enumerate(o_list):\n",
    "        o_sum = 0\n",
    "        for j, oj in enumerate(o_list):\n",
    "            if (i == j):\n",
    "                do_dnet = oi * (1 - oj)\n",
    "            else:\n",
    "                do_dnet = -oi * oj\n",
    "            o_sum += do_dnet * ds_sums[j]\n",
    "        output_deltas.append(o_sum)\n",
    "    \n",
    "    return np.tile(np.array([output_deltas]), reps=(n_inputs, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7, -1.1, 0.5]]\n",
      "[[ 0.1  0.3  0.2]\n",
      " [ 0.4  0.2 -0.7]\n",
      " [ 0.1 -0.8  0.5]]\n"
     ]
    }
   ],
   "source": [
    "filename = '../models/linear.json'\n",
    "try:\n",
    "    with open(filename, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "        \n",
    "    input_data = np.array(json_data['case']['input'])\n",
    "    target_data = np.array(json_data['case']['target'])\n",
    "    initial_weights = [np.array(layer) for layer in json_data['case']['initial_weights']]\n",
    "    n_attr = json_data[\"case\"][\"model\"][\"input_size\"]\n",
    "    n_classes = len(json_data[\"case\"][\"target\"][0])\n",
    "    learning_rate = json_data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "    \n",
    "    fnaf = FFNN(n_attr, n_classes, learning_rate)\n",
    "\n",
    "    input = json_data[\"case\"][\"input\"]\n",
    "    target = json_data[\"case\"][\"target\"]\n",
    "    \n",
    "    # with open('../data/iris.csv', newline='') as f:\n",
    "    #         reader = csv.reader(f)\n",
    "    #         data = list(reader)[1:]\n",
    "\n",
    "    \n",
    "    for i in range(len(input)):\n",
    "        fnaf.addInput(input[i],target[i])\n",
    "\n",
    "\n",
    "    # for row in data[:50]:\n",
    "    #     inputs = list(map(float, row[1:5]))\n",
    "\n",
    "    #     if row[5] == \"Iris-setosa\":\n",
    "    #         target = [1.0,0.0,0.0]\n",
    "    #     elif row[5] == \"Iris-versicolor\":\n",
    "    #         target = [0.0,1.0,0.0]\n",
    "    #     else:\n",
    "    #         target = [0.0,0.0,1.0]\n",
    "        \n",
    "    \n",
    "    #     fnaf.addInput(inputs, target)\n",
    "        \n",
    "    for i, layer_info in enumerate(json_data[\"case\"][\"model\"]['layers']):\n",
    "        layer = Layer(np.array(json_data[\"case\"][\"initial_weights\"][i]), Activation_Function(json_data[\"case\"][\"model\"]['layers'][i][\"activation_function\"]))\n",
    "        fnaf.addLayer(layer)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{filename}' was not found.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: The file '{filename}' does not contain valid JSON.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing expected key {e} in the JSON structure.\")\n",
    "        \n",
    "\n",
    "\n",
    "# fnaf = FFNN(n_attr, n_classes, learning_rate)\n",
    "\n",
    "# data = []\n",
    "\n",
    "# with open('../data/iris.csv', newline='') as f:\n",
    "#         reader = csv.reader(f)\n",
    "#         data = list(reader)[1:]\n",
    "\n",
    "# for row in data[:50]:\n",
    "#     inputs = list(map(float, row[1:5]))\n",
    "\n",
    "#     if row[5] == \"Iris-setosa\":\n",
    "#         target = [1.0,0.0,0.0]\n",
    "#     elif row[5] == \"Iris-versicolor\":\n",
    "#         target = [0.0,1.0,0.0]\n",
    "#     else:\n",
    "#         target = [0.0,0.0,1.0]\n",
    "        \n",
    "#     fnaf.addInput(inputs, target)\n",
    "\n",
    "# w_hidden = np.random.uniform(-0.5, 0.5, size=(5, 4))\n",
    "# w_out = np.random.uniform(-0.5, 0.5, size=(5, 3))\n",
    "\n",
    "# layer_hidden = Layer(w_hidden, Activation_Function.LINEAR)\n",
    "# layer_out = Layer(w_out, Activation_Function.SIGMOID)\n",
    "\n",
    "# fnaf.addLayer(layer_hidden)\n",
    "# fnaf.addLayer(layer_out)\n",
    "\n",
    "fnaf.feed_forward()\n",
    "\n",
    "print(fnaf.get_output())\n",
    "\n",
    "layers = fnaf.getLayers()\n",
    "for i in range(len(layers)):\n",
    "    print(layers[i].w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = data.iloc[:, 1:5]\n",
    "df_y = data.iloc[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df_y)\n",
    "\n",
    "# Convert integers to one-hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, onehot_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 295ms/step - loss: 0.2217 - accuracy: 1.0000\n",
      "Final weights:\n",
      "Layer 1:\n",
      "Weights:\n",
      "[[ 0.4999996   0.29999906 -0.7999995 ]\n",
      " [ 0.19999948 -0.70000094  0.40000072]]\n",
      "Biases:\n",
      "[0.19999921 0.39999843 0.10000105]\n",
      "\n",
      "Expected final weights:\n",
      "Layer 1:\n",
      "Weights:\n",
      "[[0.22, 0.36, 0.11], [0.64, 0.3, -0.89], [0.28, -0.7, 0.37]]\n",
      "Test Loss: [0.023333027958869934, 1.0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "with open('../models/linear.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Extract data from JSON\n",
    "input_data = np.array(json_data['case']['input'])\n",
    "target_data = np.array(json_data['case']['target'])\n",
    "initial_weights = [np.array(layer) for layer in json_data['case']['initial_weights']]\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential()\n",
    "for i, layer in enumerate(json_data['case']['model']['layers']):\n",
    "    if i == 0:\n",
    "        model.add(tf.keras.layers.Dense(layer['number_of_neurons'], use_bias=True, activation=layer['activation_function'], input_shape=(json_data['case']['model']['input_size'],)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(layer['number_of_neurons'], activation=layer['activation_function']))\n",
    "\n",
    "# Set initial weights\n",
    "for i, layer in enumerate(model.layers):\n",
    "    weights = initial_weights[i][1:]\n",
    "    biases = initial_weights[i][0]\n",
    "    layer.set_weights([weights, biases])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=json_data['case']['learning_parameters']['learning_rate']),\n",
    "              loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_data, target_data, epochs=json_data['case']['learning_parameters']['max_iteration'], batch_size=json_data['case']['learning_parameters']['batch_size'])\n",
    "\n",
    "# Evaluate the model\n",
    "final_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "# Check if final weights match the expected values\n",
    "# Print final weights obtained from the model\n",
    "print(\"Final weights:\")\n",
    "for i, (weights, biases) in enumerate(final_weights):\n",
    "    print(f\"Layer {i + 1}:\")\n",
    "    print(\"Weights:\")\n",
    "    print(weights)\n",
    "    print(\"Biases:\")\n",
    "    print(biases)\n",
    "\n",
    "# Print expected final weights from the JSON data\n",
    "print(\"\\nExpected final weights:\")\n",
    "for i, layer_weights in enumerate(json_data['expect']['final_weights']):\n",
    "    print(f\"Layer {i + 1}:\")\n",
    "    print(\"Weights:\")\n",
    "    print(layer_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = model.evaluate(input_data, target_data, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = model.predict(input_data, verbose=0)\n",
    "accuracy = np.mean(np.equal(np.argmax(target_data, axis=1), np.argmax(predictions, axis=1)))\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
