{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with TensorFlow Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "from activ_func import Activation_Function, reluVect, sigmoidVect, softmax\n",
    "from backprop_func import delta_linear_output, delta_relu_output, delta_sigmoid_output, delta_softmax_output, delta_linear_hidden, delta_relu_hidden, delta_sigmoid_hidden, delta_softmax_hidden\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, w: np.ndarray, activ_func: Activation_Function) -> None:\n",
    "        if (w.ndim != 2):\n",
    "            raise RuntimeError(\"Layer initialized with non 2-dimensional array\")\n",
    "\n",
    "        self.w = w\n",
    "        self.n_inputs = w.shape[0]\n",
    "        self.n_neurons = w.shape[1]\n",
    "        self.activ_func = activ_func\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    def __init__(self, n_inputs: int, n_classes: int, learning_rate: float, batch_size: int, max_iter: int, error_threshold: float, stopped_by: str) -> None:\n",
    "        self._n_inputs = n_inputs\n",
    "        self._n_classes = n_classes\n",
    "        self._batch_size = batch_size\n",
    "        self._max_iter = max_iter\n",
    "        self._error_threshold = error_threshold\n",
    "        self._use_max_iter = stopped_by == \"max_iteration\"\n",
    "\n",
    "        self._targets: list[list[float]] = []\n",
    "        self._input: list[list[float]] = []\n",
    "        self._layers: list[Layer] = []\n",
    "        self._batch_grad: list[np.ndarray[float]] = []\n",
    "\n",
    "        self._current_output: np.ndarray = None\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "\n",
    "    def init_batch_grad(self):\n",
    "        self._batch_grad = [np.zeros(shape=(layer.n_inputs, layer.n_neurons)) for layer in self._layers]\n",
    "\n",
    "    def get_output(self):\n",
    "        return np.transpose(self._current_output).tolist()\n",
    "\n",
    "    def addInput(self, newInput: list[float], target_output: list[float]):\n",
    "        if self._n_inputs != len(newInput):\n",
    "            raise RuntimeError(\"Added input with incorrect number of attributes\")\n",
    "        if self._n_classes != len(target_output):\n",
    "            raise RuntimeError(\"Added target with incorrect number of classes\")\n",
    "\n",
    "        self._input.append(newInput)\n",
    "        self._targets.append(target_output)\n",
    "\n",
    "    def addLayer(self, newLayer: Layer):\n",
    "        if len(self._input) == 0:\n",
    "            raise RuntimeError(\"Input not defined before adding hidden layer\")\n",
    "\n",
    "        if (\n",
    "            len(self._layers) == 0 and (len(self._input[0]) + 1) != newLayer.n_inputs\n",
    "        ) or (len(self._layers) != 0 and (self._layers[-1].n_neurons + 1) != newLayer.n_inputs):\n",
    "            raise RuntimeError(\n",
    "                \"Number of inputs in layer matrix does not match output from previous layer\"\n",
    "            )\n",
    "\n",
    "        self._layers.append(newLayer)\n",
    "\n",
    "    def calc_error(self, output: list[float], target: list[float], use_log: bool):\n",
    "        if use_log:\n",
    "            for idx, o_val in enumerate(output):\n",
    "                if target[idx] == 1.0:\n",
    "                    return - np.log(o_val)\n",
    "            return float('inf')\n",
    "        else:\n",
    "            err = 0.0\n",
    "            for idx, o_val in enumerate(output):\n",
    "                err += (target[idx] - o_val) ** 2\n",
    "            err /= 2.0\n",
    "            return err\n",
    "\n",
    "    def feed_forward(self):\n",
    "        for iter in range(ITER_LIMIT):\n",
    "            if iter == self._max_iter:\n",
    "                return\n",
    "\n",
    "            self.init_batch_grad()\n",
    "            iter_error = 0.0\n",
    "\n",
    "            for idx, cur_input in enumerate(self._input):\n",
    "                layer_inputs: list[list[float]] = []\n",
    "                layer_nets: list[list[float]] = []\n",
    "\n",
    "                current = np.transpose(np.array([cur_input]))\n",
    "                bias = np.array([[1.0]])\n",
    "\n",
    "                for _, layer in enumerate(self._layers):\n",
    "                    current = np.concatenate((bias, current), axis=0)\n",
    "                    layer_inputs.append(current.copy().transpose().tolist())\n",
    "\n",
    "                    new_current = np.transpose(layer.w) @ current\n",
    "                    current = new_current\n",
    "                    layer_nets.append(current.copy().transpose().tolist())\n",
    "\n",
    "                    if layer.activ_func == Activation_Function.RELU:\n",
    "                        current = reluVect(current)\n",
    "                    elif layer.activ_func == Activation_Function.SIGMOID:\n",
    "                        current = sigmoidVect(current)\n",
    "                    elif layer.activ_func == Activation_Function.SOFTMAX:\n",
    "                        current = softmax(current)\n",
    "\n",
    "                iter_error += self.calc_error(np.transpose(current).tolist()[0], self._targets[idx], layer.activ_func == Activation_Function.SOFTMAX)\n",
    "                target = self._targets[idx]\n",
    "                self._current_output = current\n",
    "                self.backwards_propagation(layer_inputs, layer_nets, target, iter, idx)\n",
    "\n",
    "                if (idx + 1) % self._batch_size == 0 or idx + 1 == len(self._input):\n",
    "                    self.update_weights()\n",
    "                    self.init_batch_grad()\n",
    "            \n",
    "            if not self._use_max_iter and iter_error <= self._error_threshold:\n",
    "                return\n",
    "\n",
    "        if not self._use_max_iter:\n",
    "            print(\"Using error_threshold to stop but hit ITER_LIMIT to stop program to run indefinitely\")\n",
    "\n",
    "    def update_weights(self):\n",
    "        for idx in range(len(self._layers)):\n",
    "            self._layers[idx].w += self._batch_grad[idx]\n",
    "\n",
    "    def update_batch_grad(self, layer_idx: int, delta: np.ndarray, layer_input: np.ndarray, hidden: bool):\n",
    "        grad = layer_input * delta * self._learning_rate\n",
    "        self._batch_grad[layer_idx] += grad\n",
    "\n",
    "    def backwards_propagation(self, layer_inputs: list[list[float]], layer_nets: list[list[float]], target: list[float], iter: int, input_idx: int):\n",
    "        ds_delta: np.ndarray = None\n",
    "        for idx, layer in enumerate(reversed(self._layers)):\n",
    "            layer_idx = (-1-idx) % len(self._layers)\n",
    "            nets = np.array(layer_nets[layer_idx]).transpose()\n",
    "\n",
    "            if idx == 0:\n",
    "                target_mat = np.array(target).transpose()\n",
    "\n",
    "                if layer.activ_func == Activation_Function.SOFTMAX:\n",
    "                    ds_delta = delta_softmax_output(self._current_output, target_mat)\n",
    "                elif layer.activ_func == Activation_Function.RELU:\n",
    "                    ds_delta = delta_relu_output(self._current_output, target_mat, nets)\n",
    "                elif layer.activ_func == Activation_Function.SIGMOID:\n",
    "                    ds_delta = delta_sigmoid_output(self._current_output, target_mat)\n",
    "                else:\n",
    "                    ds_delta = delta_linear_output(self._current_output, target_mat)\n",
    "            else:\n",
    "                cur_delta = None\n",
    "                layer_outputs = np.array([layer_inputs[layer_idx + 1][0][1:]])\n",
    "\n",
    "                if layer.activ_func == Activation_Function.SOFTMAX:\n",
    "                    cur_delta = delta_softmax_hidden(layer_outputs, ds_delta, self._layers[layer_idx + 1].w)\n",
    "                elif layer.activ_func == Activation_Function.RELU:\n",
    "                    cur_delta = delta_relu_hidden(nets, ds_delta, self._layers[layer_idx + 1].w)\n",
    "                elif layer.activ_func == Activation_Function.SIGMOID:\n",
    "                    cur_delta = delta_sigmoid_hidden(layer_outputs, ds_delta, self._layers[layer_idx + 1].w)\n",
    "                else:\n",
    "                    cur_delta = delta_linear_hidden(ds_delta, self._layers[layer_idx + 1].w)\n",
    "\n",
    "                ds_layer_input = np.array(layer_inputs[layer_idx + 1]).transpose()\n",
    "                self.update_batch_grad(layer_idx + 1, ds_delta,ds_layer_input, False)\n",
    "                ds_delta = cur_delta\n",
    "\n",
    "        ds_layer_input = np.array(layer_inputs[0]).transpose()\n",
    "        self.update_batch_grad(0, ds_delta, ds_layer_input, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def d_relu(v: float, net: float):\n",
    "    print(0 if net < 0 else v)\n",
    "    return 0 if net < 0 else v\n",
    "\n",
    "def d_relu_vect(v: np.ndarray, net: np.ndarray):\n",
    "    res = np.empty_like(v)\n",
    "    for row_idx, row in enumerate(v):\n",
    "        for col_idx, val in enumerate(row):\n",
    "            res[row_idx][col_idx] = 0 if net[row_idx][col_idx] < 0 else val\n",
    "    return res\n",
    "\n",
    "def d_softmax(o: float, net: float):\n",
    "    return 1-o if net == 1.0 else -o\n",
    "\n",
    "def d_softmax_vect(o: np.ndarray, target: np.ndarray):\n",
    "    res = np.empty_like(o)\n",
    "    for row_idx, row in enumerate(o):\n",
    "        for col_idx, o_val in enumerate(row):\n",
    "            res[row_idx][col_idx] = 1-o_val if target[col_idx] == 1.0 else -o_val\n",
    "    return res\n",
    "\n",
    "def d_sigmoid_vect(sigma_w_delta: np.ndarray, outputs: np.ndarray):\n",
    "    res = np.copy(sigma_w_delta)\n",
    "    for row_idx, row in enumerate(outputs):\n",
    "        for col_idx, o_val in enumerate(row):\n",
    "            res[row_idx][col_idx] *= o_val * (1-o_val)\n",
    "    return res\n",
    "\n",
    "\"\"\"\n",
    "All variables output, target, nets, layer_input are in the form of\n",
    "single column matrix\n",
    "\"\"\"\n",
    "\n",
    "def delta_linear_output(output: np.ndarray, target: np.ndarray):\n",
    "    output_mat = np.transpose(output)\n",
    "    target_mat = np.transpose(target)\n",
    "\n",
    "    return (target_mat - output_mat)\n",
    "\n",
    "def delta_relu_output(output: np.ndarray, target: np.ndarray, nets: np.ndarray):\n",
    "    output_mat = np.transpose(output)\n",
    "    target_mat = np.transpose(target)\n",
    "    nets_mat = np.transpose(nets)\n",
    "\n",
    "    res = d_relu_vect(target_mat - output_mat, nets_mat)\n",
    "\n",
    "    return res\n",
    "\n",
    "def delta_sigmoid_output(output: np.ndarray, target: np.ndarray):\n",
    "    output_mat = np.transpose(output)\n",
    "    target_mat = np.transpose(target)\n",
    "    \n",
    "    return ((target_mat - output_mat) * output_mat * (1 - output_mat))\n",
    "\n",
    "def delta_softmax_output(output: np.ndarray, target: np.ndarray):\n",
    "    output_mat = np.transpose(output)\n",
    "    target_mat = np.transpose(target)\n",
    "\n",
    "    return d_softmax_vect(output_mat, target_mat)\n",
    "\n",
    "def delta_linear_hidden(\n",
    "    ds_delta: np.ndarray, ds_w: np.ndarray\n",
    "):\n",
    "    sigma_delta_w = ds_delta * ds_w\n",
    "    sigma_list = np.sum(sigma_delta_w, axis=1).transpose()\n",
    "\n",
    "    return np.array([sigma_list[1:]])\n",
    "\n",
    "def delta_relu_hidden(\n",
    "    nets: np.ndarray, ds_delta: np.ndarray, ds_w: np.ndarray\n",
    "):\n",
    "    sigma_delta_w = ds_delta * ds_w\n",
    "    sigma_list = np.sum(sigma_delta_w, axis=1).transpose()\n",
    "    sigma_nets = np.array([sigma_list[1:]])\n",
    "\n",
    "    nets_mat = np.transpose(nets)\n",
    "    res = d_relu_vect(sigma_nets, nets_mat)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def delta_sigmoid_hidden(\n",
    "    output: np.ndarray, ds_delta: np.ndarray, ds_w: np.ndarray\n",
    "):\n",
    "    sigma_delta_w = ds_delta * ds_w\n",
    "    sigma_list = np.sum(sigma_delta_w, axis=1).transpose()\n",
    "    sigma_nets = np.array([sigma_list[1:]])\n",
    "\n",
    "    res = d_sigmoid_vect(sigma_nets, output)\n",
    "\n",
    "    return res\n",
    "\n",
    "# TODO fix\n",
    "def delta_softmax_hidden(\n",
    "    output: np.ndarray, ds_delta: np.ndarray, ds_w: np.ndarray\n",
    "):\n",
    "    sigma_delta_w = ds_delta * ds_w\n",
    "    sigma_list = np.sum(sigma_delta_w, axis=1).transpose()\n",
    "    sigma_delta_w_nets = np.array([sigma_list[1:]])\n",
    "\n",
    "    o_list = np.transpose(output).tolist()[0]\n",
    "\n",
    "    output_deltas = []\n",
    "    for i, oi in enumerate(o_list):\n",
    "        z = []\n",
    "        for j, oj in enumerate(o_list):\n",
    "            if i == j:\n",
    "                z.append(oi * (1 - oj))\n",
    "            else:\n",
    "                z.append(-(oi * oj))\n",
    "        z_mat = np.array([z]).transpose()\n",
    "        res_mat = z_mat @ sigma_delta_w_nets\n",
    "        output_deltas.append(res_mat[0][0])\n",
    "\n",
    "    return np.array([output_deltas])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "Layer 0\n",
      "[[ 0.1012  0.3006  0.1991]\n",
      " [ 0.4024  0.201  -0.7019]\n",
      " [ 0.1018 -0.799   0.4987]]\n",
      "\n",
      "Expected:\n",
      "Layer 0\n",
      "[[ 0.1008  0.3006  0.1991]\n",
      " [ 0.402   0.201  -0.7019]\n",
      " [ 0.101  -0.799   0.4987]]\n",
      "\n",
      "Total SSE: 9.600000000000107e-07\n",
      "FAIL: Weights yang dihasilkan belum sesuai dengan final_weights\n"
     ]
    }
   ],
   "source": [
    "MAX_SSE = 1e-7\n",
    "def calc_weights_sse(layers: list[Layer], expected: list[list[list[float]]]):\n",
    "    sse = 0.0\n",
    "    for idx, layer in enumerate(layers):\n",
    "        for row_idx, row in enumerate(layer.w):\n",
    "            for col_idx, res_w in enumerate(row):\n",
    "                sse += (res_w - expected[idx][row_idx][col_idx]) ** 2\n",
    "    return sse\n",
    "\n",
    "def readFile(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Extract data from JSON\n",
    "    input_size = json_data['case']['model']['input_size']\n",
    "    input_data = np.array(json_data['case']['input'])\n",
    "    target_data = np.array(json_data['case']['target'])\n",
    "    learning_rate = json_data['case']['learning_parameters']['learning_rate']\n",
    "    initial_weights = [np.array(layer) for layer in json_data['case']['initial_weights']]\n",
    "    n_classes = json_data['case']['model']['layers'][-1]['number_of_neurons']\n",
    "    batch_size = json_data['case']['learning_parameters']['batch_size']\n",
    "    max_iter = json_data['case']['learning_parameters']['max_iteration']\n",
    "    error_threshold = json_data['case']['learning_parameters']['error_threshold']\n",
    "    layer_config = json_data['case']['model']['layers']\n",
    "    stopped_by = json_data['expect']['stopped_by']\n",
    "    expected_weights = json_data['expect']['final_weights']\n",
    "\n",
    "    fnaf = FFNN(input_size, n_classes, learning_rate, batch_size, max_iter, error_threshold, stopped_by)\n",
    "\n",
    "    for idx, input in enumerate(input_data):\n",
    "        fnaf.addInput(input, target_data[idx])\n",
    "\n",
    "    for idx, w in enumerate(initial_weights):\n",
    "        if layer_config[idx]['activation_function'] == \"linear\":\n",
    "            fnaf.addLayer(Layer(w, Activation_Function.LINEAR))\n",
    "        elif layer_config[idx]['activation_function'] == \"relu\":\n",
    "            fnaf.addLayer(Layer(w, Activation_Function.RELU))\n",
    "        elif layer_config[idx]['activation_function'] == \"sigmoid\":\n",
    "            fnaf.addLayer(Layer(w, Activation_Function.SIGMOID))\n",
    "        else:\n",
    "            fnaf.addLayer(Layer(w, Activation_Function.SOFTMAX))\n",
    "    \n",
    "    return fnaf,expected_weights\n",
    "fnaf, expected_weights = readFile('../models/linear_small_lr.json')\n",
    "fnaf.feed_forward()\n",
    "    \n",
    "print(\"Result:\")\n",
    "for idx, layer in enumerate(fnaf._layers):\n",
    "    print(f\"Layer {idx}\")\n",
    "    print(layer.w)\n",
    "    print()\n",
    "\n",
    "print(\"Expected:\")\n",
    "for idx, w in enumerate(expected_weights):\n",
    "        print(f\"Layer {idx}\")\n",
    "        print(np.array(w))\n",
    "        print()\n",
    "\n",
    "\n",
    "sse = calc_weights_sse(fnaf._layers, expected_weights)\n",
    "print(f\"Total SSE: {sse}\")\n",
    "if sse <= MAX_SSE:\n",
    "    print(\"SUCCESS: Weights yang dihasilkan sesuai dengan final_weights\")\n",
    "else:\n",
    "        print(\"FAIL: Weights yang dihasilkan belum sesuai dengan final_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = data.iloc[:, 1:5]\n",
    "df_y = data.iloc[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df_y)\n",
    "\n",
    "# Convert integers to one-hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, onehot_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(x_train)\n",
    "X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Train MLPClassifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "mlp_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = mlp_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights:\n",
      "Layer 1:\n",
      "Weights:\n",
      "[[-0.31936002  0.48086   ]\n",
      " [ 0.424992    0.470036  ]]\n",
      "Biases:\n",
      "[0.09296    0.26138002]\n",
      "Layer 2:\n",
      "Weights:\n",
      "[[ 0.417952   -0.51584   ]\n",
      " [ 0.69251996  0.79120004]]\n",
      "Biases:\n",
      "[0.2374 0.144 ]\n",
      "\n",
      "Expected final weights:\n",
      "Layer 1:\n",
      "Weights:\n",
      "[[-0.33872, 0.46172], [0.449984, 0.440072]]\n",
      "Biases: \n",
      "[0.08592, 0.32276]\n",
      "Layer 2:\n",
      "Weights:\n",
      "[[0.435904, -0.53168], [0.68504, 0.7824]]\n",
      "Biases: \n",
      "[0.2748, 0.188]\n"
     ]
    }
   ],
   "source": [
    "with open('../models/mlp.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Extract data from JSON\n",
    "input_data = np.array(json_data['case']['input'])\n",
    "target_data = np.array(json_data['case']['target'])\n",
    "initial_weights = [np.array(layer) for layer in json_data['case']['initial_weights']]\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential()\n",
    "for i, layer in enumerate(json_data['case']['model']['layers']):\n",
    "    if i == 0:\n",
    "        model.add(tf.keras.layers.Dense(layer['number_of_neurons'], use_bias=True, activation=layer['activation_function'], input_shape=(json_data['case']['model']['input_size'],), kernel_initializer=tf.constant_initializer(initial_weights[i][1:]), bias_initializer=tf.constant_initializer(initial_weights[i][0])))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(layer['number_of_neurons'], activation=layer['activation_function'], kernel_initializer=tf.constant_initializer(initial_weights[i][1:]), bias_initializer=tf.constant_initializer(initial_weights[i][0])))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=json_data['case']['learning_parameters']['learning_rate']),\n",
    "              loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(json_data['case']['learning_parameters']['max_iteration']):\n",
    "    history = model.fit(input_data, target_data, epochs=1, batch_size=json_data['case']['learning_parameters']['batch_size'], verbose=0)\n",
    "    error = history.history['loss'][0]\n",
    "    if error < json_data['case']['learning_parameters']['error_threshold']:\n",
    "        print(f\"Training stopped at epoch {epoch+1} because error threshold reached.\")\n",
    "        break\n",
    "\n",
    "# Evaluate the model\n",
    "final_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "# Check if final weights match the expected values\n",
    "# Print final weights obtained from the model\n",
    "print(\"Final weights:\")\n",
    "for i, (weights, biases) in enumerate(final_weights):\n",
    "    print(f\"Layer {i + 1}:\")\n",
    "    print(\"Weights:\")\n",
    "    print(weights)\n",
    "    print(\"Biases:\")\n",
    "    print(biases)\n",
    "\n",
    "# Print expected final weights from the JSON data\n",
    "print(\"\\nExpected final weights:\")\n",
    "for i, layer_weights in enumerate(json_data['expect']['final_weights']):\n",
    "    print(f\"Layer {i + 1}:\")\n",
    "    print(\"Weights:\")\n",
    "    print(layer_weights[1:])\n",
    "    print(\"Biases: \")\n",
    "    print(layer_weights[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
