{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with TensorFlow Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rinaldy Adin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = data.iloc[:, 1:5]\n",
    "df_y = data.iloc[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rinaldy Adin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df_y)\n",
    "\n",
    "# Convert integers to one-hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, onehot_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551ms/step - accuracy: 1.0000 - loss: 0.2217\n",
      "Final weights:\n",
      "Layer 1:\n",
      "Weights:\n",
      "[[ 0.49999896  0.2999984  -0.7999988 ]\n",
      " [ 0.1999988  -0.7000016   0.4000014 ]]\n",
      "Biases:\n",
      "[0.19999856 0.39999777 0.10000172]\n",
      "\n",
      "Expected final weights:\n",
      "Layer 1:\n",
      "Weights:\n",
      "[[0.22, 0.36, 0.11], [0.64, 0.3, -0.89], [0.28, -0.7, 0.37]]\n",
      "Test Loss: [0.023332977667450905, 1.0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "with open('../models/linear.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Extract data from JSON\n",
    "input_data = np.array(json_data['case']['input'])\n",
    "target_data = np.array(json_data['case']['target'])\n",
    "initial_weights = [np.array(layer) for layer in json_data['case']['initial_weights']]\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential()\n",
    "for i, layer in enumerate(json_data['case']['model']['layers']):\n",
    "    if i == 0:\n",
    "        model.add(tf.keras.layers.Dense(layer['number_of_neurons'], use_bias=True, activation=layer['activation_function']))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(layer['number_of_neurons'], activation=layer['activation_function']))\n",
    "\n",
    "# Set initial weights\n",
    "for i, layer in enumerate(model.layers):\n",
    "    weights = initial_weights[i][1:]\n",
    "    biases = initial_weights[i][0]\n",
    "    layer.set_weights([weights, biases])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=json_data['case']['learning_parameters']['learning_rate']),\n",
    "              loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_data, target_data, epochs=json_data['case']['learning_parameters']['max_iteration'], batch_size=json_data['case']['learning_parameters']['batch_size'])\n",
    "\n",
    "# Evaluate the model\n",
    "final_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "# Check if final weights match the expected values\n",
    "# Print final weights obtained from the model\n",
    "print(\"Final weights:\")\n",
    "for i, (weights, biases) in enumerate(final_weights):\n",
    "    print(f\"Layer {i + 1}:\")\n",
    "    print(\"Weights:\")\n",
    "    print(weights)\n",
    "    print(\"Biases:\")\n",
    "    print(biases)\n",
    "\n",
    "# Print expected final weights from the JSON data\n",
    "print(\"\\nExpected final weights:\")\n",
    "for i, layer_weights in enumerate(json_data['expect']['final_weights']):\n",
    "    print(f\"Layer {i + 1}:\")\n",
    "    print(\"Weights:\")\n",
    "    print(layer_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = model.evaluate(input_data, target_data, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = model.predict(input_data, verbose=0)\n",
    "accuracy = np.mean(np.equal(np.argmax(target_data, axis=1), np.argmax(predictions, axis=1)))\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 0.2167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2311a7d6500>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../models/linear.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Extract data from JSON\n",
    "input_size = json_data['case']['model']['input_size']\n",
    "input_data = np.array(json_data['case']['input'])\n",
    "target_data = np.array(json_data['case']['target'])\n",
    "initial_weights = [np.array(layer) for layer in json_data['case']['initial_weights']]\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(json_data['case']['model']['input_size'],)))\n",
    "\n",
    "layers_json = json_data['case']['model']['layers']\n",
    "for i, layer_json in enumerate(layers_json):\n",
    "    layer = tf.keras.layers.Dense(units=layer_json['number_of_neurons'], activation=layer_json['activation_function'])\n",
    "\n",
    "    if (i == 0):\n",
    "        layer.build(input_shape=(input_size,))\n",
    "    else:\n",
    "        layer.build(input_shape=(layers_json[i-1]['number_of_neurons'],))\n",
    "\n",
    "    weights = np.array(initial_weights[i][1:])\n",
    "    bias = initial_weights[i][0]\n",
    "    layer.set_weights([weights, bias])\n",
    "    model.add(layer)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "model.fit(input_data, target_data, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.52      ,  0.24000001, -0.8       ],\n",
       "        [ 0.14      , -0.7866667 ,  0.46666667]], dtype=float32),\n",
       " array([0.14      , 0.31333333, 0.16666666], dtype=float32)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.summary()\n",
    "model.get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
