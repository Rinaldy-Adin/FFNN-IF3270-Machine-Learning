{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with TensorFlow Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 09:03:17.683020: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = data.iloc[:, 1:5]\n",
    "df_y = data.iloc[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df_y)\n",
    "\n",
    "# Convert integers to one-hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, onehot_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(x_train)\n",
    "X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Train MLPClassifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "mlp_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = mlp_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights:\n",
      "Layer 1:\n",
      "Weights:\n",
      "[[ 0.30110002  0.5095      0.4335    ]\n",
      " [-0.49633333 -0.9683333   0.43033332]]\n",
      "Biases:\n",
      "[-0.20366667  0.16833334  0.96166664]\n",
      "\n",
      "Expected final weights:\n",
      "Layer 1:\n",
      "Weights:\n",
      "[[0.3033, 0.5285, 0.3005], [-0.489, -0.905, 0.291]]\n",
      "Biases: \n",
      "[-0.211, 0.105, 0.885]\n"
     ]
    }
   ],
   "source": [
    "with open('../models/relu_b.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Extract data from JSON\n",
    "input_data = np.array(json_data['case']['input'])\n",
    "target_data = np.array(json_data['case']['target'])\n",
    "initial_weights = [np.array(layer) for layer in json_data['case']['initial_weights']]\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential()\n",
    "for i, layer in enumerate(json_data['case']['model']['layers']):\n",
    "    if i == 0:\n",
    "        model.add(tf.keras.layers.Dense(layer['number_of_neurons'], use_bias=True, activation=layer['activation_function'], input_shape=(json_data['case']['model']['input_size'],), kernel_initializer=tf.constant_initializer(initial_weights[i][1:]), bias_initializer=tf.constant_initializer(initial_weights[i][0])))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(layer['number_of_neurons'], activation=layer['activation_function'], kernel_initializer=tf.constant_initializer(initial_weights[i][1:]), bias_initializer=tf.constant_initializer(initial_weights[i][0])))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=json_data['case']['learning_parameters']['learning_rate']),\n",
    "              loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(json_data['case']['learning_parameters']['max_iteration']):\n",
    "    history = model.fit(input_data, target_data, epochs=1, batch_size=json_data['case']['learning_parameters']['batch_size'], verbose=0)\n",
    "    error = history.history['loss'][0]\n",
    "    if error < json_data['case']['learning_parameters']['error_threshold']:\n",
    "        print(f\"Training stopped at epoch {epoch+1} because error threshold reached.\")\n",
    "        break\n",
    "\n",
    "# Evaluate the model\n",
    "final_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "# Check if final weights match the expected values\n",
    "# Print final weights obtained from the model\n",
    "print(\"Final weights:\")\n",
    "for i, (weights, biases) in enumerate(final_weights):\n",
    "    print(f\"Layer {i + 1}:\")\n",
    "    print(\"Weights:\")\n",
    "    print(weights)\n",
    "    print(\"Biases:\")\n",
    "    print(biases)\n",
    "\n",
    "# Print expected final weights from the JSON data\n",
    "print(\"\\nExpected final weights:\")\n",
    "for i, layer_weights in enumerate(json_data['expect']['final_weights']):\n",
    "    print(f\"Layer {i + 1}:\")\n",
    "    print(\"Weights:\")\n",
    "    print(layer_weights[1:])\n",
    "    print(\"Biases: \")\n",
    "    print(layer_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 0.2167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2311a7d6500>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../models/linear.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Extract data from JSON\n",
    "input_size = json_data['case']['model']['input_size']\n",
    "input_data = np.array(json_data['case']['input'])\n",
    "target_data = np.array(json_data['case']['target'])\n",
    "initial_weights = [np.array(layer) for layer in json_data['case']['initial_weights']]\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(json_data['case']['model']['input_size'],)))\n",
    "\n",
    "layers_json = json_data['case']['model']['layers']\n",
    "for i, layer_json in enumerate(layers_json):\n",
    "    layer = tf.keras.layers.Dense(units=layer_json['number_of_neurons'], activation=layer_json['activation_function'])\n",
    "\n",
    "    if (i == 0):\n",
    "        layer.build(input_shape=(input_size,))\n",
    "    else:\n",
    "        layer.build(input_shape=(layers_json[i-1]['number_of_neurons'],))\n",
    "\n",
    "    weights = np.array(initial_weights[i][1:])\n",
    "    bias = initial_weights[i][0]\n",
    "    layer.set_weights([weights, bias])\n",
    "    model.add(layer)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "model.fit(input_data, target_data, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.52      ,  0.24000001, -0.8       ],\n",
       "        [ 0.14      , -0.7866667 ,  0.46666667]], dtype=float32),\n",
       " array([0.14      , 0.31333333, 0.16666666], dtype=float32)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.summary()\n",
    "model.get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
